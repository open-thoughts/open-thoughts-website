---
title: 'OpenThoughts3 - A new SOTA Reasoning Data Recipe'
publishedAt: '2025-06-05'
summary: 'Pushing the boundaries of open reasoning datasets through rigorous experimentation.'
---

[OpenThinker3-7B](https://huggingface.co/open-thoughts/OpenThinker3-7B) is the SOTA open-data reasoning model at its scale. Our model achieves 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25, and 54% on GPQA Diamond, representing improvements of 15.3, 17.2, and 20.5 percentage points compared to DeepSeek-R1-Distill-Qwen-7B.

All of our datasets and models are available on [openthoughts.ai](https://openthoughts.ai). We trained OpenThinker3-7B using only supervised fine-tuning, without any reinforcement learning. The key to our modelâ€™s performance is our new dataset, [OpenThoughts3-1.2M](https://huggingface.co/datasets/open-thoughts/OpenThoughts3-1.2M).  This dataset comprises 1.2 million questions across math, code, and science domains, with reasoning traces annotated from QwQ-32B. OpenThoughts3-1.2M is the result of over 1000+ rigorous experiments on each stage in the reasoning dataset construction pipeline.  


<Table
  data={{
    headers: [
      "Model",
      "HMMT",
      "AIME25", 
      "LCB 06/24-01/25",
      "CodeForces",
      "GPQA-D",
      "JEEBench"
    ],
    rows: [
      [<a href="https://huggingface.co/open-thoughts/OpenThinker3-7B">OpenThinker3-7B</a>, <b>42.7</b>, <b>53.3</b>, <b>51.7</b>, <b>32.2</b>, <b>53.7</b>, <b>72.4</b>],
      [<a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B">DS-R1-Distill-7B</a>, "25.0", "38.0", "34.5", "21.1", "33.2", "50.4"],
      [<a href="https://huggingface.co/open-r1/OpenR1-Distill-7B">OpenR1-Distill-7B</a>, "26.7", "48.0", "50.9", <b>32.9</b>, <b>52.9</b>, "70.7"],
      [<a href="https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-8B-v1">Nemotron-Nano</a>, "33.3", <b>50.7</b>, "44.3", "30.9", <b>52.9</b>, "64.3"],
      [<a href="https://huggingface.co/nvidia/AceReason-Nemotron-14B">AceReason</a>, "32.7", "47.3", "43.8", "32.5", "50.2", "55.3"],
      [<a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct</a>, "2.0", "8.0", "16.3", "10.2", "24.6", "33.9"]
    ]
  }}
/>

Our detailed report and experimental artifacts are fully public for the community to build upon together:
- [Arxiv Paper](https://arxiv.org/pdf/2506.04178)
- [Model Weights](https://huggingface.co/open-thoughts/OpenThinker3-7B)
- [Dataset](https://huggingface.co/datasets/open-thoughts/OpenThoughts3-1.2M)
- [OpenThoughts Repo](https://github.com/open-thoughts/open-thoughts) Data Code
- [Evalchemy Repo](https://github.com/mlfoundations/Evalchemy) Evaluation Code


## OpenThoughts3 Data Recipe

We extensively studied the effect of the following steps in our data generation pipeline:
1. Question sourcing
2. Question mixing
3. Question filtering
4. Generating multiple answers per question
5. Answer filtering
6. Teacher model selection

<div className="flex justify-center my-8">
  <Image
    src="/ot3_pipeline.png"
    alt="OpenThoughts3 Experimental Pipeline"
    className="rounded-lg"

  />
</div>


We build our pipeline iteratively, ablating the initial steps first and keeping the best choices fixed throughout the remaining ablations.  Our ablations contain over 1,000 controlled experiments across three data domains: math, code, and science.

Our state-of-the-art performance is driven by the insights uncovered during the experimental pipeline. These insights include:

1. Sampling multiple answers per question from a teacher model is an effective technique
to increase the size of a data source by at least 16x.

2. Models with better performance are not necessarily better teachers. QwQ-32B is a stronger
teacher than DeepSeek-R1, although it scores lower on target reasoning benchmarks such as JEEBench. 

3. We experimented with numerous verification and answer filtering methods, and none gave
significant performance improvements.

4. Selecting questions from a small number (top 1 or 2) of high-quality sources leads to better
downstream performance compared to optimizing for diversity (i.e., top 8 or 16 sources).

5. Filtering questions by LLM labeled difficulty or LLM response length yields better results
than filters typical to pre-training data curation that use embeddings or fastText


Our final pipelines for code, math, and science are presented below. 

<div className="flex justify-center my-8">
  <Image
    src="/ot3_sankey.png"
    alt="OpenThoughts3 Data Pipeline"
    className="rounded-lg"
  />
</div>

## Scaling Up

Scaling reasoning datasets is a powerful lever for increasing downstream performance, as we demonstrated scaling from Bespoke-Stratos-17k to OpenThoughts-114k and then again to OpenThoughts2-1M. With the stronger OpenThoughts3 data recipe, we once again take advantage of scale, scaling up to 1.2M samples consisting of 850,000 math, 250,000 code, and 100,000 science instruction and answer pairs. 

We outperform the previous state-of-the-art open reasoning datasets (AM and Nemotron Nano) and clearly demonstrate that more-is-more for reasoning.

<div className="flex justify-center my-8">
  <Image
    src="/ot3_scaling.png"
    alt="OpenThoughts3 Scaling Performance"
    className="rounded-lg"
  />
</div>

## Conclusion

Our OpenThinker3-7B model and associated OpenThoughts3-1.2M are the SOTA open-data model and SFT dataset for reasoning models, respectively. Our data pipeline is built on the insights gained through rigorous and comprehensive experimentation across the entire protocol.  We release all artifacts from our research and hope to fuel the research community further. We plan to further scale this pipeline up in the future and release a 32B equivalent to OpenThinker3-7B.

### Citation
<Citation>{`@misc{guha2025openthoughtsdatarecipesreasoning,
      title={OpenThoughts: Data Recipes for Reasoning Models}, 
      author={Etash Guha and Ryan Marten and Sedrick Keh and Negin Raoof and Georgios Smyrnis and Hritik Bansal and Marianna Nezhurina and Jean Mercat and Trung Vu and Zayne Sprague and Ashima Suvarna and Benjamin Feuer and Liangyu Chen and Zaid Khan and Eric Frankel and Sachin Grover and Caroline Choi and Niklas Muennighoff and Shiye Su and Wanjia Zhao and John Yang and Shreyas Pimpalgaonkar and Kartik Sharma and Charlie Cheng-Jie Ji and Yichuan Deng and Sarah Pratt and Vivek Ramanujan and Jon Saad-Falcon and Jeffrey Li and Achal Dave and Alon Albalak and Kushal Arora and Blake Wulfe and Chinmay Hegde and Greg Durrett and Sewoong Oh and Mohit Bansal and Saadia Gabriel and Aditya Grover and Kai-Wei Chang and Vaishaal Shankar and Aaron Gokaslan and Mike A. Merrill and Tatsunori Hashimoto and Yejin Choi and Jenia Jitsev and Reinhard Heckel and Maheswaran Sathiamoorthy and Alexandros G. Dimakis and Ludwig Schmidt},
      year={2025},
      eprint={2506.04178},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2506.04178}, 
}`}</Citation>