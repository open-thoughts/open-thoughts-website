---
title: 'Launching the OpenThoughts-Agent Project'
publishedAt: '2025-12-05'
summary: 'Curating the best open agent datasets.'
category: 'OpenThoughts-Agent'
---

<div className="flex justify-center my-8">
  <Image
    src="/open_thoughts_agent.png"
    alt="OpenThoughts Agent Project"
    width={600}
    height={400}
    className="rounded-lg"
  />
</div>

Today, we are announcing OpenThoughts-Agent, an open-source effort to curate the best datasets for training agents. Our first release includes datasets, models and our research codebase; [OpenThinker-Agent-v1-8B](https://huggingface.co/open-thoughts/OpenThinker-Agent-v1) is the best model of its size on Terminal-Bench 2.0 and SWE-Bench.

We built [OpenThinker-Agent-8B](https://huggingface.co/open-thoughts/OpenThinker-Agent-v1) in two stages: supervised fine-tuning, followed by reinforcement learning. Each stage required its own data pipeline – RL tasks (instructions, environments, and verifiers) and SFT traces from strong teacher agents completing tasks.

Our first set of release artifacts are:

- [OpenThinker-Agent-v1](https://huggingface.co/open-thoughts/OpenThinker-Agent-v1)
- [SFT-only Model](https://huggingface.co/open-thoughts/OpenThinker-Agent-v1-SFT)
- [OpenThoughts-Agent-v1-SFT: SFT Data (Traces)](https://huggingface.co/datasets/open-thoughts/OpenThoughts-Agent-v1-SFT)
- [OpenThoughts-Agent-v1-RL: RL Data (environments)](https://huggingface.co/datasets/open-thoughts/OpenThoughts-Agent-v1-RL)
- [Open-Thoughts-TB-Dev](https://huggingface.co/datasets/open-thoughts/OpenThoughts-TB-dev)
- [Codebase](https://github.com/open-thoughts/OpenThoughts-Agent)

OpenThoughts-Agent is an ongoing effort.

The next phase of our project involves iterating heavily on RL data recipes. We have built the training infra (integrating [Harbor](https://github.com/laude-institute/harbor) with SkyRL) for RL on the terminal bench format data. Join our [public Discord](https://discord.com/invite/6xWPKhGDbA) to get involved.

# Results

[OpenThinker-Agent-v1](https://huggingface.co/open-thoughts/OpenThinker-Agent-v1) is the best model of its size on Terminal-Bench 2.0. It's also the best on our brand new [Open-Thoughts-TB-Dev](https://huggingface.co/datasets/open-thoughts/OpenThoughts-TB-dev) benchmark, curated especially for this project (see our evals section for more). All of these benchmarks are historically very hard for small models, with even Qwen3-8B, generally a very strong model on benchmarks, achieving low accuracy.

This is just the first step. In the near future, we plan to train more models on more data, start working with larger models and MoEs, and experiment with more sophisticated data pipelines.

# Data

We are excited to release [OpenThoughts-Agent-v1-SFT](https://huggingface.co/datasets/open-thoughts/OpenThoughts-Agent-v1-SFT), our first official OpenThoughts-Agent dataset! [OpenThoughts-Agent-v1-SFT](https://huggingface.co/datasets/open-thoughts/OpenThoughts-Agent-v1-SFT) is an SFT trace dataset containing approximately 15,000 traces drawn from three different data sources we curate: nl2bash (simple synthetically generated tasks where the agent has to format shell commands effectively) and InferredBugs is a set of bugs in C# and Java collected by Microsoft that we turned into tasks.

## Instruction Sourcing

We define a task as a triplet of an instruction in the form of a markdown file, an environment defined by a DockerFile, and a verifier in the form of pytests (following [Harbor](https://github.com/laude-institute/harbor)). The verifier is not necessary for SFT data generation. All of our environments in this release are generic Ubuntu DockerFiles. To find the best way to generate instructions, we ablated 15 different approaches, selecting from both existing sources such as Nemo, SWESmith and Mind2Web, and those we created, such as StackExchange Overflow, Freelancer and Taskmaster. For each source, we generate approximately 10,000 tasks and let GPT-5-Nano solve each task once, resulting in a ~15,000 trace [SFT dataset](https://huggingface.co/datasets/open-thoughts/OpenThoughts-Agent-v1-SFT). We then evaluated each model on our new development set: [OpenThoughts-TB-Dev](https://huggingface.co/datasets/open-thoughts/OpenThoughts-TB-dev).

<div className="flex justify-center my-8">
  <Image
    src="/ota_source.png"
    alt="OTA Data Source Ablation"
    width={800}
    height={600}
    className="rounded-lg"
  />
</div>

The second design choice we ablated was the choice of teacher. One would expect that the better the teacher model is on TerminalBench, the better it performs; surprisingly, we find that this is not the case. Rather, varying teachers in the GPT model family did not improve performance, up to and including the best model on TerminalBench itself, GPT5. However, using GLM-4.6 as a teacher led to almost a 2x improvement in downstream score.

<div className="flex justify-center my-8">
  <Image
    src="/ota_teacher.png"
    alt="OTA Teacher Model Comparison"
    width={800}
    height={600}
    className="rounded-lg"
  />
</div>

## Supervised Finetuning

Similar to [OpenThoughts](https://www.openthoughts.ai), the OpenThoughts-Agent SFT pipeline utilizes a fork of the popular [Llama-Factory](https://github.com/mlfoundations/llama-factory) framework. Because we knew we would be training a lot of models, we paid a great deal of attention to optimizing our SFT pipeline to make it as efficient and precise as possible – to paraphrase Yejin Choi and Deadpool, we aimed for "maximum effort" SFT!

# Reinforcement Learning

| Model           | OpenThoughts-TB-Dev          |         | SWEBench-verified |         | TerminalBench 2.0    |       |
| --------------- | ----------------- | ------- | ----------------- | ------- | ---------- | ----- |
| SFT checkpoint  | 16.10%       |         | 14.70%       |         | 4.90% |       |
| RL'd on NL2Bash | 17.30%       | 1.20%   | 15.70%       | 1.00%   | 4.90% | 0.00% |

RL is a key part of the next phase of the OpenThoughts-Agent project. We have built an RL training stack by integrating [SkyRL](https://github.com/NovaSky-AI/SkyRL) with [Harbor](https://github.com/laude-institute/harbor) and stress-tested it with a large number of RL runs on TerminalBench-style tasks. Conducting RL on our [SFT-only model](https://huggingface.co/open-thoughts/OpenThinker-Agent-v1-SFT) using our [RL data, OpenThoughts-Agent-v1-RL](https://huggingface.co/datasets/open-thoughts/OpenThoughts-Agent-v1-RL), we get a small improvement on our [development set](https://huggingface.co/datasets/open-thoughts/OpenThoughts-TB-dev) of around ~2% and an improvement of 1% on SWE-Bench verified.

## Data

Our data is NL2Bash with synthetic verifiers. Our first step is to generate the instructions for the tasks. We first create synthetic bash tasks using GPT-5-Nano. Our environments are generic dockerfiles. We create pytests synthetically using GPT-5-Nano. We verify correctness of the tasks and verifiers using GPT-5 by removing any tasks GPT-5 gets zero reward on. This resulted in a set of 700 tasks.

As seen in the above table, doing RL on NL2Bash improves slightly over our base SFTed model. The code for generating our RL dataset is coming soon! We are excited to create new RL datasets that create even larger improvements on our benchmarks.

# Evaluation

Because Terminal-Bench 2.0 is calibrated for frontier models, small open-source models tend to struggle to get any meaningful score. Because of this, we quickly found that Terminal-Bench 2.0 wasn't the ideal benchmark for marking incremental progress with our data pipeline. Fortunately, under the supervision of Mike Merrill, Alex Shaw and Negin Raoof, and with the help of dozens of volunteer contributors as well as [Bespoke Labs](https://www.bespokelabs.ai/), we were able to curate [OpenThoughts-TB-Dev](https://huggingface.co/datasets/open-thoughts/OpenThoughts-TB-dev), a set of 70 new tasks for terminal agents. [OpenThoughts-TB-Dev](https://huggingface.co/datasets/open-thoughts/OpenThoughts-TB-dev) strongly correlates with Terminal-Bench 2.0, but it's considerably easier, making it possible for even small models to show meaningful signal.

Agentic evals are notoriously tough to interpret because of their long rollouts and highly technical content; to make our jobs easier, we produced a [trace viewer](https://dcagents-trace-viewer.replit.app/). You can use this with any of our eval or trace generation repositories to help parse the traces.

We also maintain a [live leaderboard](https://ot-agent-leaderboard.replit.app/) tracking the 300+ models we have trained so far; if you are interested in training your own models and seeing them on our leaderboard, all you need to do is join our Discord and share the public link to your model on HF.

## Build with Us!

This is not the end of the road; it's just our first signpost. As a project based on radical collaboration and openness, we need your support as we race towards the finish line.

Notebooks

Open Thoughts is a collaboration led by universities and institutes, including Stanford, UC Berkeley, UT Austin, NYU, UW, UCLA, UNC, TUM, and LAION, clusters like JSC, TACC, ALCC Perlmutter, ZIH, Oumi Exun by Lambda, and a host of supporters in the startup community, including Daytona.io, Laude Institute, Bespoke Labs and Oumi.ai.

Join our Discord to get involved!

## Citation

<Citation>{`@misc{openthoughts-agent,
  author = {Team, OpenThoughts-Agent},
  month = Dec,
  title = {{OpenThoughts-Agent}},
  howpublished = {https://open-thoughts.ai/agent},
  year = {2025}
}`}</Citation>
