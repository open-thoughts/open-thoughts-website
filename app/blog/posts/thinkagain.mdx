---
title: 'Outperforming DeepSeekR1-32B with OpenThinker2'
publishedAt: '2025-04-03'
summary: 'Announcing the next iteration of our open reasoning models and datasets.'
---
Today, we are releasing OpenThinker2-32B and OpenThinker2-7B, two new state of the art open-data reasoning models, alongside OpenThoughts2-1M, the reasoning dataset used to train these models. Our model is simply trained with SFT on our curated data, without any RL.

<Table
  data={{
    headers: [
      "Model",
      "Data",
      "AIME24",
      "AIME25",
      "AMC23",
      "MATH500",
      "GPQA-D",
      "LCBv2"
    ],
    rows: [
      [<a href="https://huggingface.co/open-thoughts/OpenThinker2-32B">OpenThinker2-32B</a>, "✅", "76.7", "58.7", "94.0", "90.8", "64.1", "72.5"],
      [<a href="https://huggingface.co/open-thoughts/OpenThinker-32B">OpenThinker-32B</a>, "✅", "68.0", "49.3", "95.5", "90.6", "63.5", "68.6"],
      [<a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B">DeepSeek-R1-Distill-Qwen-32B</a>, "❌", "74.7", "50.0", "96.5", "90.0", "65.8", "72.3"],
      [<a href="https://huggingface.co/qihoo360/Light-R1-32B">Light-R1-32B</a>, "✅", "74.7", "58.0", "96.0", "90.4", "62.0", "56.0"],
      [<a href="https://huggingface.co/simplescaling/s1.1-32B">S1.1-32B</a>, "✅", "59.3", "42.7", "91.5", "87.4", "62.0", "58.7"]
    ]
  }}
/>

<Table
  data={{
    headers: [
      "Model",
      "Data",
      "AIME24",
      "AIME25",
      "AMC23",
      "MATH500",
      "GPQA-D",
      "LCBv2"
    ],
    rows: [
      [<a href="https://huggingface.co/open-thoughts/OpenThinker2-7B">OpenThinker2-7B</a>, "✅", "50.0", "33.3", "89.5", "88.4", "49.3", "55.6"],
      [<a href="https://huggingface.co/open-thoughts/OpenThinker-7B">OpenThinker-7B</a>, "✅", "31.3", "23.3", "74.5", "83.2", "42.9", "38.0"],
      [<a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B">DeepSeek-R1-Distill-Qwen-7B</a>, "❌", "57.3", "33.3", "92.0", "89.6", "47.3", "48.4"],
      [<a href="https://huggingface.co/open-r1/OlympicCoder-7B">OlympicCoder-7B</a>, "✅", "20.7", "15.3", "63.0", "74.8", "25.3", "55.4"],
      [<a href="https://huggingface.co/open-r1/OpenR1-Qwen-7B">OpenR1-Qwen-7B</a>, "✅", "48.7", "34.7", "88.5", "87.8", "21.2", "9.5"]
    ]
  }}
/>

When we launched the Open Thoughts project, our goal was to build a SFT dataset in the open and train a DeepSeek-R1-Distill-Qwen-32B level reasoning model. We have now achieved that goal, averaged over our reasoning evaluations and we outperform DeepSeek-R1-Distill-Qwen-32B. 

We used two approaches to create OpenThoughts2-1M by adding to OpenThoughts-114K:
- Leveraging existing reasoning data from the open source community
- Sourcing and generating new code and math reasoning data

## Leveraging Existing Reasoning Data
The open source community has released a flurry of reasoning datasets in the last two months. We aimed to build on OpenThoughts-114K by adding new data from new external datasets to achieve a greater diversity and scale. We finetuned the Qwen2.5-7B-Instruct model on GeneralThought-430K, OpenR1-Math-Raw,  Llama-Nemotron-Post-Training-Dataset-v1,  SYNTHETIC-1,  KodCode-V1 and measured downstream performance on our reasoning evaluation suite. Out of the datasets that we used in these experiments, we found that OpenR1-Math performed the best overall so we include it in OpenThoughts2. 


## Generating New Reasoning Data

To further build upon the OpenThoughts-114k and OpenR1-Math mix, we generated additional math and code reasoning data. To do this, we try 26 different approaches for sourcing and generating math and code questions. For each strategy, we sample 5,000 questions, distill with DeepSeek-R1 and finetune Qwen-2.5-7B-Instruct on the resulting data. 

To determine the best data sources, we measure the downstream performance of each model on relevant reasoning benchmarks. For code sources, we measure LiveCodeBenchV2. For math sources, we measure HumanEval, MATH500, AMC23, AIME24, GPQADiamond and LiveCodeBenchV2.

<Table
  data={{
    headers: [
      "Data Source",
      "LCBv2 (%)"
    ],
    rows: [
      ["m-a-p/CodeFeedback-Filtered-Instruction", "17.6"],
      ["ajibawa-2023/Code-290k-ShareGPT", "15.9"],
      ["cognitivecomputations/dolphin-coder", "14.5"],
      ["glaiveai/glaive-code-assistant-v3", "13.5"],
      ["ise-uiuc/Magicoder-OSS-Instruct-75K", "11.7"],
      ["OpenCoder-LLM/opc-sft-stage2", "8.6"],
      ["christopher/rosetta-code", "8.6"],
      ["Magpie-Align/Magpie-Qwen2.5-Coder-Pro-300K-v0.1", "7.1"],
      ["Multilingual-Multimodal-NLP/McEval-Instruct", "6.3"],
      ["bugdaryan/sql-create-context-instruction", "5.9"],
      ["nampdn-ai/tiny-codes", "5.1"],
      ["bigcode/self-oss-instruct-sc2-exec-filter-50k", "4.9"],
      ["bigcode/commitpackft", "4.3"],
      ["cfahlgren1/react-code-instructions", "3.5"],
      ["SenseLLM/ReflectionSeq-GPT", "2.4"]
    ]
  }}
/>

<Table
  data={{
    headers: [
      "Data Source",
      "Average (%)"
    ],
    rows: [
      ["TIGER-Lab/MathInstruct", "25.1"],
      ["TIGER-Lab/MATH-plus", "24.9"],
      ["AutoMathInstruct  (ours)", "24.7"],
      ["nvidia/OpenMathInstruct-2", "24.1"],
      ["ai2-adapt-dev/openmath-2-math", "23.5"],
      ["Asap7772/hendrycks-math-mc-llama-sft-intermediate-parallel", "23.3"],
      ["ibivibiv/math_instruct", "20.5"],
      ["ddrg/named_math_formulas", "20.4"],
      ["ajibawa-2023/Maths-College", "19.2"],
      ["deepmind/math_dataset", "14.1"],
      ["allenai/math_qa", "13.2"],
      ["mathmadness/MathCoder", "9.1"]
    ]
  }}
/>

As seen in the tables below, the top performing math datasets are synthetic datasets from AutoMathInstruct, TigerLab, and Nvidia. We constructed AutoMathInstruct by searching for math related data within AutoMathText and using gpt-4o-mini to form related questions. The top performing code datasets are a mix of human coding questions (e.g. Code-290k-ShareGPT-Vicuna) and synthetic coding questions (e.g.  CodeFeedback-Filtered-Instruction).

Using 30K questions from each of the top 5 data sources for code and 12.5k questions from each of the top 4 data sources for math on top of our OpenThoughts-114k + OpenR1-Math-Raw mix, we create our final OpenThoughts2-1M dataset. 


## OpenThoughts2-1M
OpenThoughts2 is a combination of OpenThoughts-114k, verified reasoning traces from OpenR1-Math, and the questions from our best math and code sources. This is visualized in the diagram below. Our full OpenThoughts2-1M dataset is released on HuggingFace. We will soon be adding the data generation code for OpenThoughts2-1M to our github repository.

<div className="flex justify-center my-8">
  <Image
    src="/openthoughts2-1M-diagram.png"
    alt="OpenThoughts2-1M Data Curation Diagram"
    className="rounded-lg"
    width={1000}
    height={1000}
  />
</div>

## Evaluation Details
We evaluate OpenThinker2 on a set of popular reasoning benchmarks, running each benchmark multiple times (5x - AIME24, AIME25, AMC23 and 3x - LiveCodeBenchV2, GPQA-Diamond) and reporting the average accuracy. We set the temperature to 0.7 and the maximum token length to 32,768 during sampling. Our training datasets are decontaminated by removing samples with over 90% indel similarity against evaluation problems. All evaluations are conducted using our open-source framework Evalchemy, which we detailed in our previous post on reasoning evaluations.

## Conclusion
OpenThoughts2-1M is a combination of OpenThoughts-114k, OpenR1, and our newly generated data. We finetune Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct on OpenThoughts2-1M to yield OpenThinker2-7B and OpenThinker2-32B. When compared with other reasoning models created from the same base, OpenThinker2-32B outperforms all other open-data models. Since all OpenThinker models have been trained only with SFT, we expect that RL post-training can further improve their performance. We are excited for the research community to continue building together on these new reasoning models and datasets.

### Citation
<Citation>{`@misc{openthoughts,
  author = {Team, OpenThoughts},
  month = Feb,
  title = {{Open Thoughts}},
  howpublished = {https://open-thoughts.ai},
  year = {2025}
}`}</Citation>