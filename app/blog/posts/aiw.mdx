---
title: 'Testing Reasoning Robustness with Alice in Wonderland'
publishedAt: '2025-02-22'
summary: 'Reasoning models can struggle on simple reasoning puzzles, demonstrating a lack of robustness and generalization.'
---

Reasoning models achieve impressive performance on [standardized benchmarks](./measure) (AIME, MATH500, GPQA), yet show deficits in robustness and generalization on simple problems. We observe that when these reasoning models are confronted with small changes within the same problem, their performance can vary greatly. We study this phenomenon by testing models on [Alice In Wonderland](https://arxiv.org/abs/2406.02061).

<div className="flex justify-center my-8">
  <Image
    src="/aiw/aiw_1.png"
    alt="Example AIW problems"
    width={600}
    height={400}
    className="rounded-lg"
  />
</div>

Alice in Wonderland (AIW) uses simple problem templates with 6 natural variations, resulting in very similar problem instances. These problems are significantly easier to solve for humans than problems found in popular reasoning benchmarks like AIME2024, but current reasoning models still struggle and are sensitive to variations.


We test reasoning models on 3 sets of [AIW problems](https://github.com/LAION-AI/AIW/blob/main/prompts/prompts.json) (AIW Friends, AIW Plus and AIW Circle Colleagues), each containing 6 variations of the same puzzle template. 


Non-reasoning LLMs struggle significantly on these problems. Testing GPT-4o-mini on AIW, we see low performance and high variance in accuracy across different problem variations.

<div className="flex justify-center my-8">
  <Image
    src="/aiw/aiw_2.png"
    alt="gpt-4o-mini on AIW"
    width={600}
    height={400}
    className="rounded-lg"
  />
</div>

Reasoning models can achieve higher average accuracy than traditional LLMs on AIW. o1 and o1-preview achieve the highest accuracy, but DeepSeek R-1 and the larger distilled reasoning models perform better than o1-mini. Surprisingly, these larger distilled reasoning models also perform equally or better than the full DeepSeek R-1 model on the AIW tasks. 

<BarChart
  data={{
    labels: [
      "Qwen-2.5 32b Instruct",
      "gpt-4o-2024-08-06",
      "DeepSeek-R1-Distill-Qwen-1.5B",
      "Llama-3.1 70B Instruct",
      "Qwen-2.5 7b Instruct",
      "DeepSeek-R1-Distill-Qwen-7B",
      "Llama-3.1 8B Instruct", 
      "DeepSeek v3",
      "Qwen-2.5 14b Instruct",
      "Llama-3.1 405B Instruct",
      "Qwen-2.5 72b Instruct",
      "gpt-4o-2024-05-13",
      "DeepSeek-R1-Distill-Qwen-32B",
      "DeepSeek-R1-Distill-Llama-8B",
      "s1-32b",
      "o1-mini",
      "OpenThinker-32B-Unverified",
      "DeepSeek-R1-670B (37B active)",
      "OpenThinker-32B",
      "DeepSeek-R1-Distill-Llama-70B",
      "s1.1-32b",
      "Claude 3.5 Sonnet",
      "LIMO-32B",
      "o3-mini-medium",
      "o1-preview",
      "o1-2024-12-17"
    ],
    values: [
      2.6, 7.4, 7.5, 7.9, 8.3, 8.4, 9.3, 12.5, 12.7, 14.5, 15.4, 16.6, 19.5, 20.1, 
      25.4, 34.4, 44.4, 50.1, 51.1, 52.7, 54.3, 54.8, 58.1, 61.6, 88.7, 89.4
    ]
  }}
  xLabel="Model"
  yLabel="AIW Test Set Score"
  height={500}
/>

We also observe large variance due to fluctuation across problem variations, showing robustness issues even for high performing reasoning models. o1 and o1-preview, the highest performing models, show much better robustness. 

<div className="flex justify-center my-8">
  <Image
    src="/aiw/aiw_3.png"
    alt="box plot of reasoning models on AIW"
    width={1000}
    height={400}
    className="rounded-lg"
  />
</div>


Below, we show this variance across problem variations in greater detail by visualizing correct response rates with different colors for each of 6 variations of AIW Friends. Open reasoning models and o1-mini fluctuate much more than o1-preview on different problem variations. Large scale non-reasoning LLMs (Llama 3.1 405B and DeepSeek v3 671B) have very low correct response rates overall. 

<div className="flex justify-center my-8">
  <Image
    src="/aiw/aiw_4.png"
    alt="variance across problem variations"
    width={900}
    height={400}
    className="rounded-lg"
  />
</div>

In summary, we observe that open reasoning models have a severe lack of robustness to the simple reasoning problems found in Alice in Wonderland. This deficiency is not currently captured by the standard benchmarks (AIME, MATH, GPQA) used to evaluate reasoning models. There is more work to be done to close the gap between the best open and closed reasoning models.


### Citation
<Citation>{`@misc{guha2025openthoughtsdatarecipesreasoning,
      title={OpenThoughts: Data Recipes for Reasoning Models}, 
      author={Etash Guha and Ryan Marten and Sedrick Keh and Negin Raoof and Georgios Smyrnis and Hritik Bansal and Marianna Nezhurina and Jean Mercat and Trung Vu and Zayne Sprague and Ashima Suvarna and Benjamin Feuer and Liangyu Chen and Zaid Khan and Eric Frankel and Sachin Grover and Caroline Choi and Niklas Muennighoff and Shiye Su and Wanjia Zhao and John Yang and Shreyas Pimpalgaonkar and Kartik Sharma and Charlie Cheng-Jie Ji and Yichuan Deng and Sarah Pratt and Vivek Ramanujan and Jon Saad-Falcon and Jeffrey Li and Achal Dave and Alon Albalak and Kushal Arora and Blake Wulfe and Chinmay Hegde and Greg Durrett and Sewoong Oh and Mohit Bansal and Saadia Gabriel and Aditya Grover and Kai-Wei Chang and Vaishaal Shankar and Aaron Gokaslan and Mike A. Merrill and Tatsunori Hashimoto and Yejin Choi and Jenia Jitsev and Reinhard Heckel and Maheswaran Sathiamoorthy and Alexandros G. Dimakis and Ludwig Schmidt},
      year={2025},
      eprint={2506.04178},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2506.04178}, 
}`}</Citation>

<Citation>{`@article{nezhurina2024alice,
  title={Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models},
  author={Marianna Nezhurina and Lucia Cipolina-Kun and Mehdi Cherti and Jenia Jitsev},
  year={2024},
  journal={arXiv preprint arXiv:2406.02061},
  eprint={2406.02061},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}`}</Citation>
