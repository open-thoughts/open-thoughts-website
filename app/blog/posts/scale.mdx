---
title: 'Scaling up Open Reasoning with OpenThinker-32B'
publishedAt: '2025-02-12'
summary: 'Announcing OpenThinker-32B, the highest performing open data reasoning model to date.'
---
We release [OpenThinker-32B](https://huggingface.co/open-thoughts/OpenThinker-32B), a state-of-the-art open-data reasoning model. We show that powerful reasoning models can be trained by scaling data, verifying reasoning traces, and scaling model size. OpenThinker-32B outperforms existing open-data reasoning models on a host of reasoning benchmarks including math, code, and science.

<Table
  data={{
    headers: [
      "Model",
      "Dataset",
      "AIME24",
      "AIME25 I",
      "MATH500",
      "GPQA-D",
      "LCBv2"
    ],
    rows: [
      [<a href='https://huggingface.co/GAIR/LIMO'>LIMO-32B</a>, <a href="https://huggingface.co/datasets/GAIR/LIMO">0.8k</a>, "56.7", "49.3", "86.6", "58.1", "60.0"],
      [<a href='https://huggingface.co/simplescaling/s1-32B'>s1-32B</a>, <a href="https://huggingface.co/datasets/simplescaling/s1K">1k</a>, "36.0", "25.3", "84.8", "50.5", "40.9"],
      [<a href='https://huggingface.co/simplescaling/s1.1-32B'>s1.1-32B</a>, <a href="https://huggingface.co/datasets/simplescaling/s1K-1.1">1k</a>, "64.7", "49.3", "89.0", "60.1", "65.5"],
      [<a href='https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B'>R1-Distill-32B</a>, "closed", <b>76.7</b>, <b>55.9</b>, "89.4", "57.6", <b>71.2</b>],
      [<a href='https://huggingface.co/open-thoughts/OpenThinker-32B'><b>OpenThinker-32B</b></a>, <a href="https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k">114k</a>, "66.0", "53.3", <b>90.6</b>, <b>61.6</b>, "68.9"]
    ]
  }}
/>

All reported numbers are computed using our open source evaluation framework [Evalchemy](https://github.com/mlfoundations/evalchemy). Note that DeepSeek-R1-Distill-Qwen-32B is a closed data model, with a reported 800k dataset size containing 600k reasoning samples.

## Data Curation
We train [OpenThinker-32B](https://huggingface.co/open-thoughts/OpenThinker-32B) on the same [OpenThoughts-114k dataset](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k) as our earlier model [OpenThinker-7B](https://huggingface.co/open-thoughts/OpenThinker-7B). Using [DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1), we collected reasoning traces and solution attempts for a curated mix of 173k questions. We are now releasing this raw data as the [OpenThoughts-Unverfied-173k](https://huggingface.co/datasets/open-thoughts/OpenThoughts-Unverified-173k) dataset. The last step in the pipeline is filtering out samples if the reasoning trace fails verification. The [full code](https://github.com/open-thoughts/open-thoughts/tree/main/open_thoughts) which we used to construct our dataset is available on the [open-thoughts GitHub repository](https://github.com/open-thoughts/open-thoughts). 

<div className="flex justify-center my-8">
  <Image
    src="/openthoughts-114k-diagram.png"
    alt="OpenThoughts-114k Data Curation Diagram"
    className="rounded-lg"
  />
</div>

As requested by the community, we have updated the final [OpenThoughts-114k dataset](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k) to contain a "metadata" configuration that includes separate columns for:
- `problem`
- `ground_truth_solution`
- `test_cases` (code only)
- `starter_code` (code only) 
- `deepseek_reasoning`
- `deepseek_solution`
- `domain`
- `source`

The additional metadata will make it easier to use this dataset in new ways such as filtering, swapping out domains, checking verification, and changing the reasoning trace templating. 

```python
load_dataset("open-thoughts/OpenThoughts-114k", "metadata", split="train")
```

We are also excited to see the community use the problems and ground truth solutions for RL on top of the OpenThinker models, which [DeepScaleR](https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2) has shown to work particularly well at a smaller scale.

## Verification

To obtain the final [OpenThoughts-114k dataset](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k), we verify the answers and eliminate incorrect responses. As shown below, keeping the reasoning traces that fail verification can harm performance, though the unverified model still performs well compared to other 32B reasoning models. Verification serves to maintain the quality of the R1 annotations while scaling up the diversity and size of the set of training prompts. On the other hand, unverified data can be scaled more easily, which makes it worth further exploring as well.

<Table
  data={{
    headers: [
      "Model",
      "Dataset",
      "AIME24",
      "AIME25 I",
      "MATH500",
      "GPQA-D",
      "LCBv2"
    ],
    rows: [
      [<a href="https://huggingface.co/open-thoughts/OpenThinker-7B">OpenThinker-7B</a>, <a href="https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k">114k</a>, "31.3", "30.7", "84.4", "38.9", "41.8"],
      [<a href="https://huggingface.co/open-thoughts/OpenThinker-7B-Unverified">OpenThinker-7B-Unverified</a>, <a href="https://huggingface.co/datasets/open-thoughts/OpenThoughts-Unverified-173k">173k</a>, "34.0", "29.33", "83.0", "39.4", "43.8"],
      [<a href="https://huggingface.co/open-thoughts/OpenThinker-32B">OpenThinker-32B</a>, <a href="https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k">114k</a>, "66.7", "53.3", "90.6", "61.6", "68.9"],
      [<a href="https://huggingface.co/open-thoughts/OpenThinker-32B-Unverified">OpenThinker-32B-Unverified</a>, <a href="https://huggingface.co/datasets/open-thoughts/OpenThoughts-Unverified-173k">173k</a>, "60.7", "44.0", "90.0", "60.6", "69.2"]
    ]
  }}
/>

Reasoning traces for code problems are verified by [checking the solution](https://github.com/bespokelabsai/curator/tree/main/examples/bespoke-stratos-data-generation/util) attempt against existing test cases. Inspired by the challenges faced during code execution, we implemented a [code execution framework](https://github.com/bespokelabsai/curator/tree/main/examples/code-execution) in [Curator](https://github.com/bespokelabsai/curator/) that enables users to scalably and securely execute code and verify against expected outputs. Math verification is [determined by an LLM judge](https://github.com/open-thoughts/open-thoughts/blob/main/open_thoughts/math/judge.py#L19-L29) given the ground truth solution and DeepSeek-R1 solution attempt. We found that using an LLM judge instead of a stricter parsing engine ([Math-Verify](https://github.com/huggingface/Math-Verify)) for verification during data generation results in a higher yield and leads to higher performing downstream models. 

<Table
  data={{
    headers: [
      "Data Generation Verifier",
      "Dataset",
      "Evaluation Verifier",
      "AIME25 I",
      "MATH500"
    ],
    rows: [
      ["LLM judge (OpenThinker-7B)", "114k", "Hendrycks (default)", "31.3", "84.4"],
      ["LLM judge (OpenThinker-7B)", "83k", "Math-Verify", "44.0", "89.0"],
      ["Math-Verify", "114k", "Hendrycks (default)", "23.0", "55.0"],
      ["Math-Verify", "83k", "Math-Verify", "22.7", "82.2"]
    ]
  }}
/>

## Training
We finetune [Qwen2.5-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct) on [OpenThoughts-114k](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k) for 3 epochs with a 16k context length using [LLaMa-Factory](https://github.com/hiyouga/LLaMA-Factory). Our [full training configuration](https://github.com/open-thoughts/open-thoughts/blob/main/train/OpenThinker-32B.yaml) is provided in [our repository](https://github.com/open-thoughts/open-thoughts). [OpenThinker-32B](https://huggingface.co/open-thoughts/OpenThinker-32B) was trained using four 8xH100 P5 nodes over a period of 90 hours, totalling 2,880 H100 hours on Toyota Research Institute's AWS SageMaker cluster. Meanwhile, [OpenThinker-32B-Unverified](https://huggingface.co/open-thoughts/OpenThinker-32B-Unverified) was trained using 96 nodes of 4xA100 (64 GB per GPU) over a period of 30 hours, totaling 11,520 A100 hours on the Leonardo Supercomputer.

## Evaluation
We evaluate all models using our open source evaluation library [Evalchemy](https://github.com/mlfoundations/evalchemy). For AIME24 and AIME25, we average the results of five runs to compute accuracy. Our evaluation configuration uses 0.7 as the temperature, restricts the model response to 32,768 tokens, does not add any additional system or user prompts and does not use any special decoding strategy (e.g. budget forcing).

When we [launched the OpenThoughts project](https://www.open-thoughts.ai/blog/launch), we set a goal to create an open-data model that matches the performance of the official DeepSeek-R1-Distill-Qwen-32B. This gap is now almost closed. We are excited by the rapid progress in the community over the last few weeks in building open-data reasoning models and look forward to continuing building on each other's insights. 

### Citation
<Citation>{`@misc{openthoughts,
  author = {Team, OpenThoughts},
  month = jan,
  title = {{Open Thoughts}},
  howpublished = {https://open-thoughts.ai},
  year = {2025}
}`}</Citation>

