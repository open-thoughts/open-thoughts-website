---
title: 'Launching the Open Thoughts Project'
publishedAt: '2025-01-28'
summary: 'Curating the best open reasoning datasets in collaboration led by [bespokelabs](https://bespokelabs.ai/) and the [datacomp](https://www.datacomp.ai/) community.'
---
We are announcing Open Thoughts, an open-source effort to curate the best open reasoning datasets. Open Thoughts is a collaboration led by [Bespoke Labs](https://www.bespokelabs.ai/) and the [DataComp](https://www.datacomp.ai/) community from Stanford, UC Berkeley, UT Austin, UW, UCLA, UNC, TRI, and LAION. 

<div className="flex justify-center my-8">
  <Image
    src="/open_thoughts.png"
    alt="Open Thoughts Project"
    width={600}
    height={400}
    className="rounded-lg"
  />
</div>

Recent breakthroughs such as [SkyT1](https://novasky-ai.github.io/posts/sky-t1/), [STILL-2](https://arxiv.org/abs/2412.09413), and [DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1) have shown that a few hundred thousand reasoning demonstrations suffice to substantially improve the reasoning capabilities of a language model. With the release of DeepSeek-R1, such thinking demonstrations can now be synthetically created at low cost and at scale.

While this process of reasoning distillation works surprisingly well, the corresponding datasets and data generation strategies unfortunately remain closed. Moreover, there is a rich design space in data generation for reasoning that the community is only beginning to explore.

The goal of Open Thoughts is to bridge this gap and create state-of-the-art open reasoning datasets. In the process, we are publicly iterating on and sharing the best datasets and data recipes for reasoning data. We invite the community to join us to build, explore, and push the frontier of reasoning models forward together. 

<Table
  data={{
    headers: [
      "Model",
      "AIME24",
      "MATH500",
      "GPQA-D",
      "LCB Easy",
      "LCB Med",
      "LCB Hard"
    ],
    rows: [
      ["Open-Thinker-7B", "31.3", "83.0", "42.4", "75.3", "28.6", "6.5"],
      ["Bespoke-Stratos-7B", "22.7", "79.6", "38.9", "71.4", "25.2", "0.8"],
      ["DeepSeek-R1-Distill-Qwen-7B", "60.0", "88.2", "46.9", "79.7", "45.1", "14.6"],
      ["gpt-4o-2024-08-06", "8.6", "75.8", "46.5", "87.4", "42.7", "8.9"],
      ["o1-mini", "64.0", "85.6", "60.0", "92.8", "74.7", "39.8"]
    ]
  }}
/>

Today, we are also releasing our first dataset [Open-Thoughts-114k](https://huggingface.co/datasets/open-thoughts/open-thoughts-114k) and model [Open-Thinker-7B](https://huggingface.co/open-thoughts/open-thinker-7B) based on Qwen-2.5-7B-Instruct. We scaled the data strategy from [Bespoke-Stratos-17k](https://huggingface.co/datasets/bespokelabs/bespoke-stratos-17k), resulting in a significant improvement over [Bespoke-Stratos-7B](https://huggingface.co/bespokelabs/bespoke-stratos-7B). The numbers reported in the table above are evaluated with our open-source tool [Evalchemy](https://github.com/mlfoundations/evalchemy). 


We are just getting started. A new field of exciting research has just opened up. If you want to contribute or sponsor the Open Thoughts effort, [get in touch](mailto:hello@open-thoughts.ai) or raise an issue on [GitHub](https://github.com/open-thoughts/open-thoughts). Come and join us on this journey!

### Citation
```
@misc{Open Thoughts,
  author = {Open Thoughts Team},
  month = jan,
  title = {{Open Thoughts}},
  year = {2025}
}
```
