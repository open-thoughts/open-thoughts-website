---
title: Open Thoughts Project
publishedAt: 2024-02-20
summary: Curating the best open reasoning datasets
---

<div className="flex justify-center my-8">
  <Image
    src="/open_thoughts.png"
    alt="Open Thoughts Project"
    width={600}
    height={400}
    className="rounded-lg"
  />
</div>

A [DataComp](https://www.datacomp.ai/) and [Bespoke Labs](https://bespokelabs.ai/) community effort to curate the best open reasoning datasets.

Our first goal is to curate a reasoning dataset to train state of the art small reasoning models that surpass [DeepSeek-R1-Distill-32B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B) and [DeepSeek-R1-Distill-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B) on math and code reasoning benchmarks.

{/* ### News */}
{/* *Jan 28, 2025*: We launched the Open Thoughts project and [OpenThoughts-114k dataset](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k) */}

{/* *Jan 22, 2025*: We [released](https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation) our [Bespoke-Stratos-17k dataset](https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k), [Bespoke-Stratos-7B model](https://huggingface.co/bespokelabs/Bespoke-Stratos-7B) , [Bespoke-Stratos-32B model](https://huggingface.co/bespokelabs/Bespoke-Stratos-32B) */}

## Latest 32B Results

<Table
  data={{
    headers: [
      "Model",
      "Data",
      "AIME24",
      "AIME25",
      "AMC23",
      "MATH500",
      "GPQA-D",
      "LCBv2"
    ],
    rows: [
      [<a href="https://huggingface.co/open-thoughts/OpenThinker2-32B"><b>OpenThinker2-32B</b></a>, "✅", <b>76.7</b>, <b>58.7</b>, "94.0", <b>90.8</b>, "64.1", <b>72.5</b>],
      [<a href="https://huggingface.co/open-thoughts/OpenThinker-32B"><b>OpenThinker-32B</b></a>, "✅", "68.0", "49.3", "95.5", "90.6", "63.5", "68.6"],
      [<a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B">R1-Distill-32B</a>, "❌", "74.7", "50.0", <b>96.5</b>, "90.0", <b>65.8</b>, "72.3"],
      [<a href="https://huggingface.co/qihoo360/Light-R1-32B">Light-R1-32B</a>, "✅", "74.7", "58.0", "96.0", "90.4", "62.0", "56.0"],
      [<a href="https://huggingface.co/simplescaling/s1.1-32B">S1.1-32B</a>, "✅", "59.3", "42.7", "91.5", "87.4", "62.0", "58.7"]
    ]
  }}
/>

## Latest 7B Results

<Table
  data={{
    headers: [
      "Model",
      "Data",
      "AIME24",
      "AIME25",
      "AMC23",
      "MATH500",
      "GPQA-D",
      "LCBv2"
    ],
    rows: [
      [<a href="https://huggingface.co/open-thoughts/OpenThinker2-7B"><b>OpenThinker2-7B</b></a>, "✅", "50.0", "33.3", "89.5", "88.4", <b>49.3</b>, <b>55.6</b>],
      [<a href="https://huggingface.co/open-thoughts/OpenThinker-7B"><b>OpenThinker-7B</b></a>, "✅", "31.3", "23.3", "74.5", "83.2", "42.9", "38.0"],
      [<a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B">R1-Distill-7B</a>, "❌", <b>57.3</b>, "33.3", <b>92.0</b>, <b>89.6</b>, "47.3", "48.4"],
      [<a href="https://huggingface.co/open-r1/OlympicCoder-7B">OlympicCoder-7B</a>, "✅", "20.7", "15.3", "63.0", "74.8", "25.3", "55.4"],
      [<a href="https://huggingface.co/open-r1/OpenR1-Qwen-7B">OpenR1-Math-7B</a>, "✅", "48.7", <b>34.7</b>, "88.5", "87.8", "21.2", "9.5"]
    ]
  }}
/>

The numbers reported in the table above are evaluated with our open-source tool [Evalchemy](https://github.com/mlfoundations/evalchemy). Our models are trained on [OpenThoughts-114k](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k) and [OpenThoughts2-1M](https://huggingface.co/datasets/open-thoughts/OpenThoughts2-1M). Data generation code is available on [Github](https://github.com/open-thoughts/open-thoughts).

## About us

We are a team of researchers and engineers from Stanford, University of California Berkeley, University of Washington, Bespoke Labs, UT Austin, Juelich Supercomputing Center (JSC), LAION, UCLA, UNC Chapel Hill, and Toyota Research Institute united around building the best datasets (and thus the best models). See our previous works at [datacomp.ai](https://www.datacomp.ai/) and [mlfoundations](https://github.com/mlfoundations).

Open Thoughts is supported by [Bespoke Labs](https://www.bespokelabs.ai/), [NSF IFML](https://www.ifml.institute/),  [UT Austin Machine Learning Lab](https://ml.utexas.edu/), [Juelich Supercomputing Center](https://www.fz-juelich.de/en/ias/jsc), [Toyota Research Institute](https://www.tri.global/), [Lambda Labs](https://lambdalabs.com/).

## Announcements
